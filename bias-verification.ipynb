{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7082010,"sourceType":"datasetVersion","datasetId":2673949}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-11T04:51:56.139946Z","iopub.execute_input":"2024-10-11T04:51:56.140425Z","iopub.status.idle":"2024-10-11T04:51:56.594551Z","shell.execute_reply.started":"2024-10-11T04:51:56.140370Z","shell.execute_reply":"2024-10-11T04:51:56.593349Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Import the datasets\nbase = pd.read_csv('/kaggle/input/bank-account-fraud-dataset-neurips-2022/Base.csv')\nvariant_1 = pd.read_csv('/kaggle/input/bank-account-fraud-dataset-neurips-2022/Variant I.csv')\nvariant_2 = pd.read_csv('/kaggle/input/bank-account-fraud-dataset-neurips-2022/Variant II.csv')\nvariant_3 = pd.read_csv('/kaggle/input/bank-account-fraud-dataset-neurips-2022/Variant III.csv')\nvariant_4 = pd.read_csv('/kaggle/input/bank-account-fraud-dataset-neurips-2022/Variant IV.csv')\nvariant_5 = pd.read_csv('/kaggle/input/bank-account-fraud-dataset-neurips-2022/Variant V.csv')\n\n# Put all datasets in a dictionary for easy iteration\ndatasets = {\n    \"Base\": base,\n    \"Variant I\": variant_1,\n    \"Variant II\": variant_2,\n    \"Variant III\": variant_3,\n    \"Variant IV\": variant_4,\n    \"Variant V\": variant_5,\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:51:56.597186Z","iopub.execute_input":"2024-10-11T04:51:56.597694Z","iopub.status.idle":"2024-10-11T04:52:44.195322Z","shell.execute_reply.started":"2024-10-11T04:51:56.597621Z","shell.execute_reply":"2024-10-11T04:52:44.193971Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import resample\n\ndatasets = {\n    \"Base\": base,\n    \"Variant I\": variant_1,\n    \"Variant II\": variant_2,\n    \"Variant III\": variant_3,\n    \"Variant IV\": variant_4,\n    \"Variant V\": variant_5,\n}\n\ntrain_test_data = {}\n\n# Process each dataset\nfor name, data_raw in datasets.items():\n    print(f\"Processing Dataset: {name}\")\n    \n    # Check column names\n\n    # Verify if target column exists\n    target_variable = 'fraud_bool'\n    if target_variable not in data_raw.columns:\n        raise ValueError(f\"'{target_variable}' column not found in {name} dataset. Please check the column name.\")\n\n    # Balance the dataset by downsampling the majority class\n    df_fraud = data_raw[data_raw[target_variable] == 1]\n    df_non_fraud = data_raw[data_raw[target_variable] == 0]\n    \n    df_non_fraud_downsampled = resample(df_non_fraud,\n                                        replace=False,\n                                        n_samples=len(df_fraud),\n                                        random_state=42)\n    \n    # Combine the minority and downsampled majority class\n    df_balanced = pd.concat([df_fraud, df_non_fraud_downsampled])\n    \n    # Shuffle the dataset and reset the index\n    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    # Verify that 'month' column exists\n    if 'month' not in df_balanced.columns:\n        raise ValueError(f\"'month' column not found in {name} dataset. Please check the column name.\")\n\n    # Split the data into train and test based on the \"month\" feature\n    train_data = df_balanced[df_balanced['month'].between(0, 5)]\n    test_data = df_balanced[df_balanced['month'].between(6, 7)]\n    \n    # Drop the \"month\" feature from train and test sets\n    train_data = train_data.drop(columns=['month'])\n    test_data = test_data.drop(columns=['month'])\n    \n    # Reset index\n    train_data.reset_index(drop=True, inplace=True)\n    test_data.reset_index(drop=True, inplace=True)\n    \n    # Separate features and target variable\n    X_train = train_data.drop(columns=[target_variable])\n    y_train = train_data[target_variable]\n    X_test = test_data.drop(columns=[target_variable])\n    y_test = test_data[target_variable]\n\n    # Correctly identify numerical and categorical features after dropping 'month'\n    numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n    \n    # Check feature columns before pre-processing\n    \n    # Update ColumnTransformer with correct feature names\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', StandardScaler(), numerical_features),\n            ('cat', OneHotEncoder(sparse_output=False, drop='first'), categorical_features)\n        ]\n    )\n    \n    # Apply the ColumnTransformer\n    try:\n        X_train_transformed = preprocessor.fit_transform(X_train)\n        X_test_transformed = preprocessor.transform(X_test)\n    except KeyError as e:\n        print(f\"KeyError: {e}\")\n        print(f\"Numerical Features: {numerical_features}\")\n        print(f\"Categorical Features: {categorical_features}\")\n        raise ValueError(\"A given column is not a column of the dataframe. Check the feature definitions.\")\n    \n    # Handle class imbalance using SMOTE\n    smote = SMOTE(random_state=42)\n    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_transformed, y_train)\n    \n    # Store transformed datasets\n    train_test_data[name] = {\n        'X_train': X_train_resampled,\n        'X_test': X_test_transformed,\n        'y_train': y_train_resampled,\n        'y_test': y_test,\n        'X_test_raw': X_test,  # Save raw data for bias analysis\n        'preprocessor': preprocessor  # Save preprocessor for inverse transform or further processing\n    }\n\nprint(\"Data Transformation Completed for All Variants.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:52:44.196950Z","iopub.execute_input":"2024-10-11T04:52:44.197409Z","iopub.status.idle":"2024-10-11T04:52:49.076737Z","shell.execute_reply.started":"2024-10-11T04:52:44.197356Z","shell.execute_reply":"2024-10-11T04:52:49.075485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# Define models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n    \"KNN\": KNeighborsClassifier(),\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"Naive Bayes\": GaussianNB(),\n}\n\ntrained_models = {}\n\n# Train models for each dataset\nfor dataset_name, data in train_test_data.items():\n    for model_name, model in models.items():\n        print(f\"Training {model_name} on {dataset_name} dataset...\")\n        \n        # Train the model\n        model.fit(data['X_train'], data['y_train'])\n        \n        # Store the trained model\n        trained_models[(dataset_name, model_name)] = model\n","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:52:49.078230Z","iopub.execute_input":"2024-10-11T04:52:49.078730Z","iopub.status.idle":"2024-10-11T04:53:19.264764Z","shell.execute_reply.started":"2024-10-11T04:52:49.078689Z","shell.execute_reply":"2024-10-11T04:53:19.263397Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fairlearn","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:53:19.267332Z","iopub.execute_input":"2024-10-11T04:53:19.267811Z","iopub.status.idle":"2024-10-11T04:53:34.998118Z","shell.execute_reply.started":"2024-10-11T04:53:19.267767Z","shell.execute_reply":"2024-10-11T04:53:34.996247Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_auc_score\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n\n# Lists to store test and bias results\nall_metrics_results = []\nbias_metrics_results = []\n\n# Iterate through each dataset and model to test them\nfor dataset_name, data in train_test_data.items():\n    for model_name, model in models.items():\n        print(f\"Training and Evaluating {model_name} on {dataset_name} dataset...\")\n\n        # Train the model\n        model.fit(data['X_train'], data['y_train'])  # Train on the resampled training data\n\n        # Make predictions\n        y_test_pred = model.predict(data['X_test'])\n        y_test_prob = model.predict_proba(data['X_test'])[:, 1] if hasattr(model, \"predict_proba\") else None\n        \n        # Calculate metrics\n        cm = confusion_matrix(data['y_test'], y_test_pred)\n        report = classification_report(data['y_test'], y_test_pred, output_dict=True)\n        roc_auc = roc_auc_score(data['y_test'], y_test_prob) if y_test_prob is not None else 'N/A'\n\n        # Print Confusion Matrix\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n        plt.title(f\"Confusion Matrix for {model_name} on {dataset_name} Dataset\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Actual\")\n        plt.show()\n\n        # Print Classification Report and ROC-AUC\n        print(f\"Classification Report for {model_name} on {dataset_name} Dataset:\")\n        print(classification_report(data['y_test'], y_test_pred))\n        if y_test_prob is not None:\n            print(f\"ROC-AUC Score for {model_name} on {dataset_name} Dataset: {roc_auc:.2f}\")\n        else:\n            print(f\"ROC-AUC Score for {model_name} on {dataset_name} Dataset: Not Applicable\")\n\n        # Calculate performance metrics\n        accuracy = accuracy_score(data['y_test'], y_test_pred)\n        f1 = f1_score(data['y_test'], y_test_pred)\n        roc_auc = roc_auc_score(data['y_test'], y_test_prob) if y_test_prob is not None else 'N/A'\n\n        # Fairness metrics evaluation\n        sensitive_attribute = 'employment_status'\n        if sensitive_attribute not in data['X_test_raw'].columns:\n            raise ValueError(f\"'{sensitive_attribute}' column not found in {dataset_name} dataset for bias evaluation.\")\n\n        sensitive_features = data['X_test_raw'][sensitive_attribute]\n\n        # Calculate Demographic Parity Difference\n        dp_diff = demographic_parity_difference(data['y_test'], y_test_pred, sensitive_features=sensitive_features)\n\n        # Calculate Equalized Odds Difference\n        eo_diff = equalized_odds_difference(data['y_test'], y_test_pred, sensitive_features=sensitive_features)\n\n        # Store all metrics results\n        all_metrics_results.append({\n            'Dataset': dataset_name,\n            'Model': model_name,\n            'Accuracy': accuracy,\n            'F1 Score': f1,\n            'ROC-AUC Score': roc_auc,\n            'Demographic Parity Difference': dp_diff,\n            'Equalized Odds Difference': eo_diff\n        })\n\n        # Store bias metrics separately for plotting\n        bias_metrics_results.append({\n            'Dataset': dataset_name,\n            'Model': model_name,\n            'Demographic Parity Difference': dp_diff,\n            'Equalized Odds Difference': eo_diff\n        })\n\n# Convert all metrics results to DataFrame for visualization\nall_metrics_df = pd.DataFrame(all_metrics_results)\nbias_summary = pd.DataFrame(bias_metrics_results)\n\n# Print All Metrics Summary\nprint(\"\\nAll Metrics Summary:\")\nprint(all_metrics_df)\n\n# Plotting All Metrics Summary\nfig, axs = plt.subplots(1, 3, figsize=(24, 8))\n\n# Plot Accuracy for all datasets and models\nsns.barplot(data=all_metrics_df, x='Dataset', y='Accuracy', hue='Model', ax=axs[0], palette='viridis')\naxs[0].set_title('Accuracy by Dataset and Model')\naxs[0].set_ylabel('Accuracy')\naxs[0].set_xlabel('Dataset')\naxs[0].tick_params(axis='x', rotation=45)\naxs[0].grid(axis='y')\n\n# Plot F1 Score for all datasets and models\nsns.barplot(data=all_metrics_df, x='Dataset', y='F1 Score', hue='Model', ax=axs[1], palette='magma')\naxs[1].set_title('F1 Score by Dataset and Model')\naxs[1].set_ylabel('F1 Score')\naxs[1].set_xlabel('Dataset')\naxs[1].tick_params(axis='x', rotation=45)\naxs[1].grid(axis='y')\n\n# Plot ROC-AUC Score for all datasets and models (if applicable)\nsns.barplot(data=all_metrics_df[all_metrics_df['ROC-AUC Score'] != 'N/A'], x='Dataset', y='ROC-AUC Score', hue='Model', ax=axs[2], palette='plasma')\naxs[2].set_title('ROC-AUC Score by Dataset and Model')\naxs[2].set_ylabel('ROC-AUC Score')\naxs[2].set_xlabel('Dataset')\naxs[2].tick_params(axis='x', rotation=45)\naxs[2].grid(axis='y')\n\nplt.tight_layout()\nplt.show()\n\n# Plotting Bias Metrics\nfig, axs = plt.subplots(1, 2, figsize=(18, 8))\n\n# Plot Demographic Parity Difference for all datasets and models\nsns.barplot(data=bias_summary, x='Dataset', y='Demographic Parity Difference', hue='Model', ax=axs[0], palette='Blues')\naxs[0].set_title('Demographic Parity Difference by Dataset and Model')\naxs[0].set_ylabel('Demographic Parity Difference')\naxs[0].set_xlabel('Dataset')\naxs[0].tick_params(axis='x', rotation=45)\naxs[0].grid(axis='y')\n\n# Plot Equalized Odds Difference for all datasets and models\nsns.barplot(data=bias_summary, x='Dataset', y='Equalized Odds Difference', hue='Model', ax=axs[1], palette='Greens')\naxs[1].set_title('Equalized Odds Difference by Dataset and Model')\naxs[1].set_ylabel('Equalized Odds Difference')\naxs[1].set_xlabel('Dataset')\naxs[1].tick_params(axis='x', rotation=45)\naxs[1].grid(axis='y')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:53:35.000807Z","iopub.execute_input":"2024-10-11T04:53:35.001287Z","iopub.status.idle":"2024-10-11T04:54:24.672191Z","shell.execute_reply.started":"2024-10-11T04:53:35.001243Z","shell.execute_reply":"2024-10-11T04:54:24.670920Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}